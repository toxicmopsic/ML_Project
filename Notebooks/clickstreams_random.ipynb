{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pv2wSvo5sBx3",
        "outputId": "027cbe45-92f8-4604-ef49-18863d3a5be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lifestream\n",
            "  Downloading pytorch-lifestream-0.5.2.tar.gz (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lifestream) (9.0.0)\n",
            "Collecting pytorch-lightning==1.6.*\n",
            "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.*\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1.2\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (4.65.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (3.19.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (2.11.2)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (1.13.1+cu116)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (23.0)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (1.22.4)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.*->pytorch-lifestream) (2023.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.*->pytorch-lifestream) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.*->pytorch-lifestream) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.*->pytorch-lifestream) (3.10.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.4,>=2.2\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (2.16.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (67.6.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (0.40.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (1.51.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (0.6.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.*->pytorch-lifestream) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.*->pytorch-lifestream) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.*->pytorch-lifestream) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.*->pytorch-lifestream) (2022.12.7)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.*->pytorch-lifestream) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (6.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.*->pytorch-lifestream) (3.2.2)\n",
            "Building wheels for collected packages: pytorch-lifestream, antlr4-python3-runtime\n",
            "  Building wheel for pytorch-lifestream (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lifestream: filename=pytorch_lifestream-0.5.2-py3-none-any.whl size=256980 sha256=38571a181597a1dca413a5c06c6346347b6ade4fd3208c927073dc14f047f9c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/3c/f7/565354c0a36c9f9cd0084237bcb8d36697554d25cc7d9a1ae1\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=a26eaf2d4b62a3460cfc516b6b287c412abfafa9b4f06fccf2b220aa9ecba0e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "Successfully built pytorch-lifestream antlr4-python3-runtime\n",
            "Installing collected packages: tokenizers, antlr4-python3-runtime, pyDeprecate, omegaconf, multidict, frozenlist, async-timeout, yarl, torchmetrics, hydra-core, huggingface-hub, aiosignal, transformers, aiohttp, pytorch-lightning, pytorch-lifestream\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 frozenlist-1.3.3 huggingface-hub-0.13.3 hydra-core-1.3.2 multidict-6.0.4 omegaconf-2.3.0 pyDeprecate-0.3.2 pytorch-lifestream-0.5.2 pytorch-lightning-1.6.5 tokenizers-0.13.2 torchmetrics-0.11.4 transformers-4.27.2 yarl-1.8.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !pip install pytorch-lifestream"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch"
      ],
      "metadata": {
        "id": "4nn9Vk0pspEb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('data/train_matching.csv'):\n",
        "    ! mkdir -p data\n",
        "    ! curl -OL https://storage.yandexcloud.net/datasouls-ods/materials/acfacf11/train_matching.csv\n",
        "    # ! unzip -j -o clickstream.zip '*.csv' -d data\n",
        "    ! mv train_matching.csv data/\n",
        "\n",
        "if not os.path.exists('data/education.csv'):\n",
        "    ! mkdir -p data\n",
        "    ! curl -OL https://storage.yandexcloud.net/datasouls-ods/materials/e756bf99/train.csv\n",
        "    #! unzip -j -o transactions.zip '*.csv' -d data\n",
        "    ! mv train.csv data/\n",
        "\n",
        "\n",
        "print(f'Loaded csv files')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFlF3hpZsYrv",
        "outputId": "fbe1d422-29b2-40c4-86c7-934892ac6926"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1045k  100 1045k    0     0   683k      0  0:00:01  0:00:01 --:--:--  683k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  307k  100  307k    0     0   285k      0  0:00:01  0:00:01 --:--:--  285k\n",
            "Loaded csv files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hk8E6mqwwxzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f4cc5c-a6c3-4f82-a117-fe60512ba990"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -av \"/content/drive/My Drive/ml/dataset_clickstream\" \"dataset_clickstream\""
      ],
      "metadata": {
        "id": "EqSZ03zFwzy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594f4fd0-7379-4f90-df3d-833b8d30cfe4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/My Drive/ml/dataset_clickstream' -> 'dataset_clickstream'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open ('dataset_clickstream', 'rb') as ds:\n",
        "    dataset_clickstreams = pickle.load(ds)"
      ],
      "metadata": {
        "id": "t4dK9lL1w6Tz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding"
      ],
      "metadata": {
        "id": "bOtVh4E_C77S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_clickstreams = sorted(dataset_clickstreams, key=lambda x: x['user_id'])"
      ],
      "metadata": {
        "id": "XQIk6yp4C_FN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(dataset_clickstreams, test_size=0.2, random_state=42)\n",
        "\n",
        "len(train), len(test)"
      ],
      "metadata": {
        "id": "8KbMv-BwELyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f1d1e9-f678-494c-c518-968d1c672d19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5083, 1271)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[0].keys()"
      ],
      "metadata": {
        "id": "4zEsuBltZ9bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02210464-a676-4f16-f934-e22fb4d30f72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['user_id', 'new_uid', 'event_time', 'cat_id'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "KvNVC-nqfeQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
        "from ptls.frames.coles import CoLESModule\n",
        "\n",
        "trx_encoder_params = dict(\n",
        "    norm_embeddings = False,\n",
        "    embeddings_noise=0.003,\n",
        "    numeric_values={},\n",
        "    embeddings={\n",
        "        'cat_id': {'in': 200, 'out': 48},\n",
        "        'new_uid': {'in': 100, 'out': 24}\n",
        "    },\n",
        ")\n",
        "\n",
        "seq_encoder = RnnSeqEncoder(\n",
        "    trx_encoder=TrxEncoder(**trx_encoder_params),\n",
        "    hidden_size=256,\n",
        "    type='gru',\n",
        "    bidir=False,\n",
        "    trainable_starter='static'\n",
        ")\n",
        "\n",
        "model = CoLESModule(\n",
        "    seq_encoder=seq_encoder,\n",
        "    optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
        "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9),\n",
        ")"
      ],
      "metadata": {
        "id": "jJ3pmKQ1zPeD"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ptls.data_load.datasets import MemoryMapDataset\n",
        "from ptls.data_load.iterable_processing import SeqLenFilter\n",
        "from ptls.frames.coles import ColesDataset\n",
        "from ptls.frames.coles.split_strategy import SampleSlices\n",
        "from ptls.frames import PtlsDataModule\n",
        "\n",
        "train_dl = PtlsDataModule(\n",
        "    train_data=ColesDataset(\n",
        "        MemoryMapDataset(\n",
        "            data=train,\n",
        "            i_filters=[\n",
        "                SeqLenFilter(min_seq_len=25),\n",
        "            ],\n",
        "        ),\n",
        "        splitter=SampleSlices(\n",
        "            split_count=5,\n",
        "            cnt_min=25,\n",
        "            cnt_max=200,\n",
        "        ),\n",
        "    ),\n",
        "    train_num_workers=16,\n",
        "    train_batch_size=256,\n",
        ")"
      ],
      "metadata": {
        "id": "kK71AUV4fm2P"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import logging\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=15,\n",
        "    gpus=1 if torch.cuda.is_available() else 0,\n",
        "    enable_progress_bar=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbNJRDYWfs8w",
        "outputId": "5c7fcf73-3221-45eb-9f0b-b42a29eca9cc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True, used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding inference\n",
        "\n",
        "from ptls.data_load.datasets import inference_data_loader\n",
        "\n",
        "train_dl = inference_data_loader(train, num_workers=4, batch_size=64)\n",
        "train_embeds = torch.vstack(trainer.predict(model, train_dl, ))\n",
        "\n",
        "test_dl = inference_data_loader(test, num_workers=4, batch_size=64)\n",
        "test_embeds = torch.vstack(trainer.predict(model, test_dl))\n",
        "\n",
        "train_embeds.shape, test_embeds.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd7NjMfJitqH",
        "outputId": "532328b8-76f1-4df5-f5df-19a5fcac0786"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5083, 256]), torch.Size([1271, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifacation"
      ],
      "metadata": {
        "id": "qDtz4RARcYzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path='data/'"
      ],
      "metadata": {
        "id": "2gtMJDi9m32H"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_matching = pd.read_csv(os.path.join(data_path, 'train_matching.csv'))"
      ],
      "metadata": {
        "id": "_QQlF--74Uh4"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join target and embeddings\n",
        "\n",
        "df_target = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
        "df_target = df_target.merge(train_matching, on='bank')\n",
        "df_target = df_target[['rtk', 'higher_education']]\n",
        "df_target.rename(columns={\"rtk\": \"user_id\"}, inplace=True)\n",
        "df_target = df_target.set_index('user_id')\n",
        "df_target.rename(columns={\"higher_education\": \"target\"}, inplace=True)\n",
        "\n",
        "train_df = pd.DataFrame(data=train_embeds, columns=[f'embed_{i}' for i in range(train_embeds.shape[1])])\n",
        "train_df['user_id'] = [x['user_id'] for x in train]\n",
        "# train_df = train_df.merge(df_target, how='left', on='user_id')\n",
        "train_df = train_df.merge(df_target, on='user_id')\n",
        "\n",
        "test_df = pd.DataFrame(data=test_embeds, columns=[f'embed_{i}' for i in range(test_embeds.shape[1])])\n",
        "test_df['user_id'] = [x['user_id'] for x in test]\n",
        "# test_df = test_df.merge(df_target, how='left', on='user_id')\n",
        "test_df = test_df.merge(df_target, on='user_id')\n",
        "print(train_df.shape, test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB-Kui_XmlIN",
        "outputId": "e8ec9bd3-30f9-46cc-e1f3-4a37c4b5727b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5083, 258) (1271, 258)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "embed_columns = [x for x in train_df.columns if x.startswith('embed')]\n",
        "x_train, y_train = train_df[embed_columns], train_df['target']\n",
        "x_test, y_test = test_df[embed_columns], test_df['target']\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)"
      ],
      "metadata": {
        "id": "Azj2FjIzmtLK"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "print(\"accuracy score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"precision score:\", precision_score(y_test, y_pred))\n",
        "print(\"recall score:\", recall_score(y_test, y_pred))\n",
        "print(\"f1 score:\", f1_score(y_test, y_pred))\n",
        "print(\"roc auc_score:\", roc_auc_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csR8I0qjdJ7g",
        "outputId": "8e632364-1ee7-43c2-9b7b-2a00038c0753"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score: 0.6939417781274587\n",
            "precision score: 0.7275042444821732\n",
            "recall score: 0.9264864864864865\n",
            "f1 score: 0.8150261531145983\n",
            "roc auc_score: 0.49937041087330103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5p5zZzQiolHy"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}